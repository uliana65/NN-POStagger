{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "import data_loader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc26e73",
   "metadata": {},
   "source": [
    "# DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a220390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.de.vec: 5.97GB [07:07, 13.9MB/s]                                                                    \n",
      "  0%|                                                                                      | 0/2275233 [00:00<?, ?it/s]Skipping token b'2275233' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2275233/2275233 [04:02<00:00, 9397.50it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, dev_dataloader, test_dataloader, vocabulary, tagset, pretrained_embeddings = data_loader.load(\"UD_German-GSD-master/de_gsd-ud-train.conllu\", \"UD_German-GSD-master/de_gsd-ud-dev.conllu\", \"UD_German-GSD-master/de_gsd-ud-test.conllu\")\n",
    "\n",
    "# create embeddings with size (length_vocabulary, 300)\n",
    "embedding = torch.nn.Embedding(num_embeddings=len(vocabulary), embedding_dim=300)\n",
    "embedding.weight.data = pretrained_embeddings\n",
    "embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca542fe4",
   "metadata": {},
   "source": [
    "# SAMPLE DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b440b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': tensor([   58,   282,   443,     3,  2300, 20144,     4,   806,     3,    61,\n",
       "           948,    45,   171,  4091,    57,     2]),\n",
       " 'tags': tensor([7, 6, 0, 1, 6, 0, 2, 0, 1, 7, 5, 8, 8, 0, 3, 1])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataloader.dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0ddf27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sehr',\n",
       " 'gute',\n",
       " 'beratung',\n",
       " ',',\n",
       " 'schnelle',\n",
       " 'behebung',\n",
       " 'der',\n",
       " 'probleme',\n",
       " ',',\n",
       " 'so',\n",
       " 'stelle',\n",
       " 'ich',\n",
       " 'mir',\n",
       " 'kundenservice',\n",
       " 'vor',\n",
       " '</s>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.lookup_tokens(list(sample[\"sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636f2aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADV',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADV',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset.lookup_tokens(list(sample[\"tags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8712d3",
   "metadata": {},
   "source": [
    "# MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc1b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    A POS tagger for the German sentences.\n",
    "    Predicts POS tag for a tensor of embeddings of words.\n",
    "    A 2 layer Feedforward Neural Network.\n",
    "    \"\"\"\n",
    "    \n",
    "    # params of the model\n",
    "    def __init__(self,\n",
    "                input_size: int,\n",
    "                number_tags: int,\n",
    "                hidden_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_size: the size of the input layer - length (n. of cols) of an input tensor; must be equal to embedding length\n",
    "        :param number_tags: the length of POS tag set - length (n. of cols) of an output;\n",
    "        :param hidden_size: the size of the hidden layer\n",
    "        \"\"\"\n",
    "        super(NNTagger, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True) #[inp=300, out=300]; hidden_size - number of hidden units\n",
    "        self.layer2 = nn.Linear(in_features=hidden_size, out_features=number_tags) #[inp=300, out=18]\n",
    "        \n",
    "        \n",
    "    # forward function\n",
    "    def forward(self, \n",
    "                input_sent):\n",
    "        \"\"\"\n",
    "        Gets an input batch and feeds it through the layers of the network to get the output predictions for each input\n",
    "        :param input_sent: a batch of tensors corresponding to sentences; shape: (batch_size, sentence_length, embedding_length)\n",
    "        :return: the predictions of the model; shape: (batch_size, sentence_length, number_tags)\n",
    "        \"\"\"\n",
    "        # feed input batch into the lstm layer\n",
    "        out_lstm_layer, _ = self.layer1(input_sent)\n",
    "\n",
    "        # feed output of lstm layer to the linear layer\n",
    "        out_linear_layer = self.layer2(out_lstm_layer)\n",
    "\n",
    "        return out_linear_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522aa80",
   "metadata": {},
   "source": [
    "# MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23811881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model to train\n",
    "\n",
    "hidden_size = 300\n",
    "input_size = 300\n",
    "num_tags = len(tagset) #18 pos tags\n",
    "\n",
    "tagger = NNTagger(input_size=input_size, number_tags=num_tags, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6300e",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146094e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Parameters ###\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.Adam\n",
    " \n",
    "loss_function = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32020aff",
   "metadata": {},
   "source": [
    "# TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc673e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module, \n",
    "                train_data, dev_data,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float,\n",
    "                embedding) -> None:\n",
    "    \"\"\"\n",
    "    Does one commplete training run for N epochs\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs\n",
    "    :param optimizer_type: the type of optimizer\n",
    "    :param loss_function: the type of loss function\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :param embedding: embedding object with size (vocab_length, 300)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # brings model into training mode\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # runs training for specified number of epochs\n",
    "    for epoch in tqdm(range(num_epochs), desc='POS Tagger Training\\n'):\n",
    "        \n",
    "        print(f'---------- Started Epoch {epoch+1} -----------')\n",
    "\n",
    "        for sent in train_data:\n",
    "            \n",
    "            # transforms one-hot encoding into embedding tensor\n",
    "            input_sent = torch.stack([embedding.weight.data[word] for word in sent[0][0]])\n",
    "            # wraps into one more tensor to keep the right dimentionality: (batch_size=1, sent_length, embedding_length)\n",
    "            input_sent = torch.stack([input_sent])\n",
    "\n",
    "            # computes model predictions with current model parameters\n",
    "            model_output = model(input_sent)\n",
    "\n",
    "            # computes Loss for current sent\n",
    "            loss = loss_function(input=model_output[0], target=sent[0][1])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # updates parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_acc, dev_acc = evaluate(tagger, [train_data, dev_data])\n",
    "        #experiment.log_metric(\"dev_accuracy\", dev_acc, epoch=epoch)\n",
    "        #experiment.log_metric(\"train_accuracy\", train_acc, epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a24c8",
   "metadata": {},
   "source": [
    "# EVALUATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eef92688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brings model in evaluation mode\n",
    "def evaluate(model, eval_set):\n",
    "    tagger.eval()\n",
    "    \n",
    "    correct_preds = [0 for ds in eval_set]\n",
    "    num_tokens = [0 for ds in eval_set]\n",
    "    final_accs = [0 for ds in eval_set]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ds in range(len(eval_set)):\n",
    "            for sent in eval_set[ds]:\n",
    "\n",
    "                # model output of size (sent_length, num_tags), logit vals\n",
    "                input_sent = torch.stack([embedding.weight.data[word] for word in sent[0][0]])\n",
    "                input_sent = torch.stack([input_sent])\n",
    "                sent_pred = tagger(input_sent)\n",
    "                sent_pred = torch.sigmoid(sent_pred)\n",
    "                # prediction vector of size (1, pred_tag_inds)\n",
    "                sent_pred = torch.argmax(sent_pred, dim=2)\n",
    "\n",
    "                # target vec (1, tag_inds)\n",
    "                target_vec = sent[0][1]\n",
    "                num_tokens[ds] += len(target_vec)\n",
    "\n",
    "                # gets accuracy on the whole data\n",
    "                correct_preds[ds] += torch.sum(sent_pred == target_vec)\n",
    "        \n",
    "            final_accs[ds] = (correct_preds[ds] / num_tokens[ds]) * 100\n",
    "    \n",
    "            print(f\"ACCURACY: {torch.round(final_accs[ds])}% in {ds} ds\")\n",
    "    return final_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee81d8c",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "POS Tagger Training\n",
      ":   0%|                                                                     | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Start Training ------------\n",
      "---------- Started Epoch 1 -----------\n"
     ]
    }
   ],
   "source": [
    "# runs a complete training loop\n",
    "#for sent in train_dataloader:\n",
    "train_model(model=tagger, train_data=train_dataloader, dev_data=dev_dataloader, num_epochs=epochs, \n",
    "            optimizer_type=optimizer, loss_function=loss_function, learning_rate=learning_rate, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a82a89",
   "metadata": {},
   "source": [
    "# EVALUATION ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee378c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tagger, [test_dataloader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29209ea4",
   "metadata": {},
   "source": [
    "# SAVE AND LOAD MODEL PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e699f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model parameters in a .pt file\n",
    "torch.save(tagger.state_dict(), \"./model_parameters_tagger10e300FM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4967a192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model parameters again\n",
    "\n",
    "# first create an instance of the model class\n",
    "tagger = NNTagger(input_size=input_size, number_tags=num_tags, hidden_size=hidden_size)\n",
    "\n",
    "# then load the trained parameters\n",
    "tagger.load_state_dict(torch.load(\"./model_parameters_tagger10e300FM.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdfce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
